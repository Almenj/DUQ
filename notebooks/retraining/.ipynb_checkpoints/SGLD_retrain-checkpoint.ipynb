{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812df3b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'post'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11708\\3513864327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../duq/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msgld\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mSG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'post'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../duq/')  \n",
    "import post, pre\n",
    "import sgld as SG\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppresses sklearn warning about PCA.. Is this ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234                  # Assign a value to the seed\n",
    "pre.set_seed(seed)      # Set the seed for 'random', 'np.random', 'torch.manual_seed' and 'torch.cuda.manual_seed_all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60933b74",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b9dbe",
   "metadata": {},
   "source": [
    "## Import, Pre-process and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde120a",
   "metadata": {},
   "source": [
    "## Splitting Test Set from Main Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4accb3b",
   "metadata": {},
   "source": [
    "This code imports the full dataset in `csv` format for analysis. \n",
    "\n",
    "The user can choose which frequency to predict by changing `y_cols` here. More than one frequency can be in this list, however some of the plotting functions need to be changed slightly. See `MC_Dropout_multi_output.ipynb` for an example of predicting multiple frequencies. \n",
    "\n",
    "The column headings (and corresponding indices) are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087edf34",
   "metadata": {},
   "source": [
    "| num_x | num_y | num_z | width_x | width_y | width_z | freq1 | freq2 | freq3 | freq4 | freq5 | freq6 |\n",
    "| :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- |\n",
    "| [0] | [1] | [2] | [3] | [4] | [5] | [6] | [7] | [8] | [9] | [10] | [11] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = [6]             # Which column(s) does the the dependent variable(s) that we're interested in sit? list\n",
    "x_cols = [0,1,2,3,4,5]   # Which column(s) is the independent variable(s) (features) in? list\n",
    "component = \"n/a\"        # this is used if we want to sort and split the data by a particular parameter\n",
    "\n",
    "# Import the csv file and save as DataFrame\n",
    "filepath = '../data/all_data.csv'\n",
    "df_orig = pd.read_csv(filepath, header=None)\n",
    "df_orig.columns = ['nbays_x', 'nbays_y', 'nbays_z', 'bay_width_x', 'bay_width_y', 'bay_width_z', 'modal_freq_1', 'modal_freq_2', 'modal_freq_3', 'modal_freq_4', 'modal_freq_5', 'modal_freq_6']\n",
    "\n",
    "# Calculate the mean and standard deviation of the original dataset\n",
    "data_mean = df_orig.mean()\n",
    "data_std = df_orig.std()\n",
    "\n",
    "# Visualise full dataset in reduced dimensional space \n",
    "_, components = post.PCA_transformdata(df_orig.iloc[:,x_cols], return_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d95ce",
   "metadata": {},
   "source": [
    "## Split Data into Test, Train, Val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e54f97",
   "metadata": {},
   "source": [
    "All data within the specified `train_lims_all` gets put into a training set. Then a proportion `val_split` of the training set gets put into a validation set. \n",
    "\n",
    "Anything within the regions specified by `ood_lims_in` and `ood_lims_out` goes in the inner OOD and outer OOD regions respectively. These are then combined to make a test set. \n",
    "\n",
    "The datasets are then normalised and an additional column is added that contains the L2 distance of each point from the geometric median point. \n",
    "\n",
    "Each input parameter (i.e. columns 1 to 6) is plotted in reduced dimensional space for visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b582a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_lims_in = [[0., 3.], [0., 3.], [0., 3.], [0., 5.], [0., 5.], [0., 5.]]\n",
    "train_lims_all = [[3., 10], [3., 10], [3., 10], [5., 10], [5., 10], [5., 10]]\n",
    "ood_lims_out = [[10, 25], [10, 25], [10, 30], [10, 12], [10, 12], [10, 12]]\n",
    "\n",
    "TRAIN, VAL, TEST = pre.split_by_bounds(df=df_orig,\n",
    "                                 x_cols=x_cols,\n",
    "                                 y_cols=y_cols,\n",
    "                                 train_lims_all=train_lims_all,\n",
    "                                 ood_lims_in=ood_lims_in,\n",
    "                                 ood_lims_out=ood_lims_out,\n",
    "                                 data_mean=data_mean, \n",
    "                                 data_std=data_std, \n",
    "                                 PCA_components=components,\n",
    "                                 val_split=0.1,\n",
    "                                 verbose=True,\n",
    "                                 plots=True,\n",
    "                                 figsize=(10,10),\n",
    "                                      save_image=True, savename=\"../reports/final_report_images/splitting_data/manual_bounds.eps\", saveformat=\"eps\")\n",
    "\n",
    "x_train, y_train, train_data, train_indices = TRAIN\n",
    "x_val, y_val, val_data, val_indices = VAL\n",
    "x_test, y_test, test_data, test_indices = TEST\n",
    "\n",
    "y_train_unnorm = pre.unnormalise(y_train,data_mean[y_cols].values,data_std[y_cols].values)\n",
    "y_val_unnorm = pre.unnormalise(y_val,data_mean[y_cols].values,data_std[y_cols].values)\n",
    "y_test_unnorm = pre.unnormalise(y_test,data_mean[y_cols].values,data_std[y_cols].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2950b",
   "metadata": {},
   "source": [
    "## Run quick stats on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2d102",
   "metadata": {},
   "source": [
    "These histograms show the distribution of y values (i.e. frequency) in the split datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e202480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,constrained_layout=True, figsize=(15,5))\n",
    "\n",
    "sns.distplot(y_train_unnorm,bins=50,label=\"Train\",ax=axs[0])\n",
    "sns.distplot(y_val_unnorm,bins=50,label=\"Val\",ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of y Values in Split Data\\nTrain and Validation Only\")\n",
    "axs[0].set_xlabel(\"First Modal Frequency (Hz)\")\n",
    "axs[0].legend()\n",
    "\n",
    "sns.distplot(y_train_unnorm,label=\"Train\",ax=axs[1])\n",
    "sns.distplot(y_test_unnorm,label=\"Test\",ax=axs[1])\n",
    "sns.distplot(y_val_unnorm,label=\"Val\",ax=axs[1])\n",
    "axs[1].set_title(\"Distribution of y Values in Split Data\\nTest, Train and Validation\")\n",
    "axs[1].set_xlabel(\"First Modal Frequency (Hz)\")\n",
    "axs[1].set_ylim((0,0.5))\n",
    "axs[1].set_xlim((-0.5,25))\n",
    "axs[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27bfd4",
   "metadata": {},
   "source": [
    "# Neural Network Instantiation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdd600",
   "metadata": {},
   "source": [
    "## Define Parameters of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12038a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model so we can load it and perform inference later\n",
    "#savename = \"../trained_models/for_prediction/MC_35\"\n",
    "checkpoint_path = \"../trained_models/for_retraining/SGLD\"\n",
    "\n",
    "# Define the wandb parameters\n",
    "parameters = dict(\n",
    "    # Specific to this method\n",
    "    noise_multiplier=1,\n",
    "    anneal_gamma = None,\n",
    "    burnin_epochs=250,\n",
    "    num_networks=150,\n",
    "    \n",
    "    # Generic Hyperparameters\n",
    "    num_epochs= 1000,\n",
    "    batch_size= 50,# len(train_data), # Batch size for training data\n",
    "    lr=  1e-3,                   # Learning rate\n",
    "    weight_decay= None,          # Weight decay\n",
    "    \n",
    "    # Model architecture\n",
    "    input_dim= len(x_cols),      # Number of input neurons\n",
    "    output_dim= len(y_cols),     # Number of output neurons\n",
    "    num_units= 50,              # Number of neurons per hidden layer\n",
    "    num_layers=4,\n",
    "    \n",
    "    # Data\n",
    "    y_cols = y_cols,             # Which column(s) contain the dependent variable(s) / label(s) \n",
    "    x_cols = x_cols,             # Which column(s) contain the independent variable(s) / feature(s)\n",
    "    \n",
    "    # Logigng only\n",
    "    component = None,       # Which parameter are we sorting by (as an int)? \n",
    "    sortby=None,       # Name of the component we're sorting by (as a string)\n",
    "    model_name= \"SGLD_Final\",    # For logging only\n",
    "    criterion_name= \"MSELoss\",   # For logging only\n",
    "    optimiser_name= \"Adam\",      # For logging only \n",
    "    cutoff_percentile = None,  # How much we're splitting from top and bottom of sorted training set for test set \n",
    "    val_split=None,         # How much we're splitting from the train set (minus the test set), as a float between 0-1\n",
    "    seed=seed,                   # Random seed used (for logging only)\n",
    ")\n",
    "\n",
    "assert parameters['output_dim'] == len(y_cols), f\"Please ensure that the number of output neurons is correct! There should be {len(y_cols)}\"\n",
    "assert parameters['input_dim'] == len(x_cols), f\"Please ensure that the number of input neurons is correct! There should be {len(x_cols)}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2229ec6",
   "metadata": {},
   "source": [
    "## If using Weights and Biases API, Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a176df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Whether or not to log the run to Weights and Biases (https://wandb.ai/home). Requires an account\n",
    "wandb_mode = False\n",
    "\n",
    "if wandb_mode: \n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    wandb.init(config=parameters, entity=\"archieluxton\", project=parameters['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406d1c8",
   "metadata": {},
   "source": [
    "## Instantiate the Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model class of type MC Dropout\n",
    "model = SG.SGLD(train_data = train_data,\n",
    "                              parameters=parameters, \n",
    "                              val_data=val_data,\n",
    "                              data_mean=data_mean,\n",
    "                              data_std=data_std,\n",
    "                              wandb_mode=wandb_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ceb526",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "net, train_loss, val_loss = model.train_model(LLP=True,                        # Live loss plot, defulat: True\n",
    "                                              checkpoint=True,                 # Must be True to continue training later\n",
    "                                              checkpoint_path=checkpoint_path) # Savename for the trained model checkpoint\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end-start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbddee",
   "metadata": {},
   "source": [
    "# Perform Forward Passes for each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b14c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, means_train, stds_train, y_train_np = model.run_sampling(x_train, y_train)\n",
    "samples_test, means_test, stds_test, y_test_np = model.run_sampling(x_test, y_test)             \n",
    "samples_val, means_val, stds_val, y_val_np = model.run_sampling(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dbc5f",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Post-Processing and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8420db",
   "metadata": {},
   "source": [
    "## Count number of 'untrustworthy' predictions\n",
    "This function checks how many of the true values in each dataset (test, train, val) fall outside the confidence interval.\n",
    "\n",
    "It does *not* tell us how accurate the mean predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0aaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train = post.count_wrong_preds(samples_train, y_train_np, 1, \"SD\", False)\n",
    "err_test = post.count_wrong_preds(samples_test, y_test_np, 1, \"SD\", False)\n",
    "err_val = post.count_wrong_preds(samples_val, y_val_np, 1, \"SD\", False)\n",
    "\n",
    "print(f\"Untrustworthy in train:\\t{np.sum(err_train)}\\t(low: {err_train[0]}, high: {err_train[1]})\")\n",
    "print(f\"Untrustworthy in test:\\t{np.sum(err_test)}\\t(low: {err_test[0]}, high: {err_test[1]})\")\n",
    "print(f\"Untrustworthy in val:\\t{np.sum(err_val)}\\t(low: {err_val[0]}, high: {err_val[1]})\")\n",
    "print(\"----------------------\")\n",
    "print(f\"Total untrustworthy:\\t{np.sum(err_train)+np.sum(err_test)+np.sum(err_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a117c",
   "metadata": {},
   "source": [
    "## Plot the Uncertainty of Each Datapoint (Train, Test and Val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae10449",
   "metadata": {},
   "source": [
    "The plot below shows us intuitively what the accuracy and uncertainty of predictions in each dataset is. From this, it's clear to see that the prediction accuracy is high in the training and validation set, and both the accuracy and certainty in the test set are low as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abce95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uncert_plot = post.result_plots(all_samples = [samples_val, samples_train, samples_test],\n",
    "             labels = [\"Validation\", \"Train\", \"Test (OoD)\"],\n",
    "             output_labels = [\"freq1\", \"freq2\", \"freq3\"],\n",
    "             all_true = [y_val_np, y_train_np, y_test_np],\n",
    "             output_num=0,\n",
    "             #true_inds = [val_indices, train_indices, test_indices],\n",
    "             interval = 1,\n",
    "             method=\"SD\",\n",
    "             component_name = parameters[\"component\"],\n",
    "             sort=True,\n",
    "             sortby=\"0\",\n",
    "             bar_method=\"bars\",\n",
    "             title=\"SGLD, Tuned Model\",\n",
    "             ylabel=\"First Modal Frequency (Hz)\",\n",
    "              figsize=(15,10.5),\n",
    "             ylim=(-0.5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e74327",
   "metadata": {},
   "source": [
    "### Ridgeline Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87665b",
   "metadata": {},
   "source": [
    "This visualises the distribution for each point prediction made. Every eighth datapoint is omitted for clarity of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a414726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joypy\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "means_train2, samples_train2, stds_train2 = post.sort_data([means_train, samples_train, stds_train],sortby=0) \n",
    "skip_every = 8\n",
    "data = pd.DataFrame(samples_train2.squeeze()[:,::skip_every])\n",
    "\n",
    "fig, axes = joypy.joyplot(pd.DataFrame(samples_train2.squeeze()[:,::skip_every]),\n",
    "                          range_style='own', # Limits each subplot to the area where it's non-zero\n",
    "                          #ylabels=False,   # Also turns off horz grid lines if False\n",
    "                          #xlabels=True,\n",
    "                          overlap=0.5,        \n",
    "                          grid='y',        # 'y': Horz grid lines, True: horz and vert\n",
    "                          linewidth=0.5,     # Outlines\n",
    "                          #labels = labels,   # Which y labels to plot (corresponding to horz lines). Doesn't work for some reason\n",
    "                          kind='counts',   # Plotting raw counts rather than estimated density\n",
    "                          #legend=True,   \n",
    "                          figsize=(8,15),\n",
    "                          bins=50, \n",
    "                          fade=True,       # subplots get progressively higher alpha values \n",
    "                          title=f\"Ridge Plot of Training set\\nSkipping every {skip_every} point(s):\\n{data.shape[1]} samples of {len(means_train[0])})\",\n",
    "                          colormap=cm.autumn_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234c7b3",
   "metadata": {},
   "source": [
    "## Show the Distribution of Errors and Uncertainties in each Dataset\n",
    "### Using Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba565506",
   "metadata": {},
   "source": [
    "These plots make a range of summaries about the dataset and predictions.\n",
    "\n",
    "In row major ordering:\n",
    "\n",
    " - Plot 1: The error between each prediction sample and the true value. This outlines the accuracy of the predictions, and the spread of the samples.  \n",
    " - Plot 2: Very similar to Plot 1, except looking at the mean of all samples drawn for each datapoint. This outlines the accuracy of the predictions. \n",
    " - Plot 3: Shows the absolute width of the confidence interval of every data point in each set. This outlines the uncertainty of the predictions.  \n",
    " - Plot 4: Shows the distribution of the predicted values in each dataset. This outlines the performance of the model as well as describing the dataset.\n",
    " - Plot 5: If a confidence interval does not capture the true y value, then how far outside the confidence interval is the y value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070190ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "samples = [samples_val, samples_train, samples_test]\n",
    "true = [y_val_np, y_train_np, y_test_np]\n",
    "labels=[\"Validation\", \"Train\", \"Test (OoD)\"]\n",
    "histograms1, statistics1 = post.histogram_stats(samples,\n",
    "                                                true,\n",
    "                                                labels,\n",
    "                                                method=\"SD\",\n",
    "                                                interval=1,\n",
    "                                                dp=.3,\n",
    "                                                bins=100,\n",
    "                                                figsize=(13,9),\n",
    "                                                plot_1_ylim=(0,2),\n",
    "                                                plot_1_xlim=(0,20),\n",
    "                                                plot_2_ylim=(0,2),\n",
    "                                                plot_3_ylim=(0,25),\n",
    "                                                plot_3_xlim=(0,0.5),\n",
    "                                                plot_2_xlim=(-20,10),\n",
    "                                                plot_5_ylim=(0,1),\n",
    "                                                plot_5_xlim=(-40,10))\n",
    "                                                \n",
    "\n",
    "# Save the image to weights and biases\n",
    "if wandb_mode: wandb.log({\"Histograms_output0\":wandb.Image(histograms1)})\n",
    "if wandb_mode: wandb.log({\"Data_output0\":statistics1})\n",
    "    \n",
    "    \n",
    "# Print the individual figures shown in the histograms\n",
    "# import json\n",
    "# print(json.dumps(statistics1, indent=6, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370cc774",
   "metadata": {},
   "source": [
    "### Using Prinicpal Component Analysis in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb062b58",
   "metadata": {},
   "source": [
    "This intuitively shows both the distribution of test/train/validation data, as well as the uncertainty of the predictions in each of those sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf1f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "x_train_PCA, components = post.PCA_transformdata(x_train, return_components=True)\n",
    "x_test_PCA = post.PCA_transformdata(x_test, components=components)\n",
    "x_val_PCA = post.PCA_transformdata(x_val, components=components)\n",
    "\n",
    "post.PCA_plot([x_train_PCA, x_test_PCA, x_val_PCA], \n",
    "              [stds_train, stds_test, stds_val],\n",
    "              labels=[\"Train\", \"Test\", \"Val\"],\n",
    "              output_num= 0,\n",
    "              figsize=(8,8),\n",
    "              scalingfactor=10,\n",
    "              scalingpower=2,\n",
    "              produce_animation=False,\n",
    "              legend_num=5,\n",
    "              legend_HDI=0.99,\n",
    "              dpi = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b1381",
   "metadata": {},
   "source": [
    "# Making Predictions on Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7132fe",
   "metadata": {},
   "source": [
    "## Forward Passes of a Single Datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c80e1",
   "metadata": {},
   "source": [
    "Here, we can make a single prediction on an arbitrary datapoint. The function returns the array of samples, a mean and a standard deviation from that mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b4845",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_samples = 150\n",
    "x_value = np.array([5, 5, 5, 10, 10, 10])\n",
    "\n",
    "s, m, sd = model.make_prediction(x_value = x_value,\n",
    "                          model = model,\n",
    "                          data_mean = data_mean,\n",
    "                          data_std = data_std,\n",
    "                          verbose=False,            # Optional, default=False\n",
    "                          plots=False,              # Optional, default=False\n",
    "                          num_samples=num_samples)  # Optional, default = 100\n",
    "\n",
    "print(f\"Using X = {x_value}, modal frequency = {m.item():.3f} Hz, +/- {sd.item():.3f} Hz (+/-{100*sd.item()/m.item():.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985e626",
   "metadata": {},
   "source": [
    "## Check Uncertainties of in-domain vs OOD Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d98a5",
   "metadata": {},
   "source": [
    "Here, we can run a range of predictions over a given domain and visualise the resulting uncertainty, both in absolute terms and in relative uncertainty ($\\sigma$/$\\mu$).\n",
    "\n",
    " - *params* is laid out like: `[num\\_x, num\\_y, num\\_z, dim\\_x, dim\\_y, dim\\_z]`.\n",
    " - To plot a param as constant, set it to e.g. `[5]` for 5.\n",
    " - To plot a param as a range, set it to e.g. `[1, 50, 10]` to do `np.linspace(1, 50, 10)`.\n",
    " - Can only set 3 as ranges at a time! Axes automatically get renamed to accomodate the change\n",
    "\n",
    "Note that the complexity scales with the cube of the number of samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70f14e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "params = [[1, 20, 10], [1, 20, 10], [1, 50, 10], [5], [5], [5]]\n",
    "\n",
    "# all_domain: Draws the cube that indicates the training domain. Same format as above.\n",
    "all_domain = [[3, 10], [3, 10], [3, 10], [5, 10], [5, 10], [5, 10]]\n",
    "\n",
    "num_samples=20               # How many forward passes to do per datapoint to get the posterior distribution\n",
    "figsize = (10, 10)           # Figure size\n",
    "\n",
    "means_pred, stds_pred, percent_uncert, xxx, yyy, zzz, labels, ranges, consts = post.generate_3d_samples(model,\n",
    "                             params=params,\n",
    "                             num_samples=num_samples)     # Optional, defualt=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e5f4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline  \n",
    "post.generate_3d_plot(means_pred,                  # Mean prediction values of every point in cube\n",
    "                 stds_pred,                   # Std of every point in cube\n",
    "                 percent_uncert,              # Percent uncertainty every point in cube\n",
    "                 xxx, yyy, zzz,               # Meshgrid for x, y and z\n",
    "                 labels=labels,               # Labels for x, y and z axes. E.g. [\"dim_x\", \"dim_y\", \"dim_z\"]\n",
    "                 ranges=ranges,               # Which parameter numbers are set as ranges\n",
    "                 consts=consts,               # Which parameter numbers are constant\n",
    "                 all_domain = all_domain,     # Draw the cube that represents the training domain\n",
    "                 scale=False,                  # Scale all the axes to maintain relative size \n",
    "                 mode=\"absolute\",             # \"absolute\": SD values. \"relative\": 100*(SD/mu). Default=\"absolute\"\n",
    "                 produce_animation=False,     # Default=False\n",
    "                 animation_name=\"_.mp4\",      # Default=\"./animation.mp4\"\n",
    "                 figsize=(8,8),             # Optional, default=(15,15)\n",
    "                 scalingpower=3,              # Raise SD to this power when using \"absolute\" to scale size. Optional, default=3\n",
    "                 scalingscalar=10)            # Multiply SD by this before raising to power. Optional, default=10\n",
    "\n",
    "post.generate_3d_plot(means_pred,                  # Mean prediction values of every point in cube\n",
    "                 stds_pred,                   # Std of every point in cube\n",
    "                 percent_uncert,              # Percent uncertainty every point in cube\n",
    "                 xxx, yyy, zzz,               # Meshgrid for x, y and z\n",
    "                 ranges=ranges,               # Which parameter numbers are set as ranges\n",
    "                 consts=consts,               # Which parameter numbers are constant\n",
    "                 all_domain = all_domain,     # Draw the cube that represents the training domain\n",
    "                 labels=labels,               # Labels for x, y and z axes. E.g. [\"dim_x\", \"dim_y\", \"dim_z\"]\n",
    "                 scale=False,                 # Scale all the axes to maintain relative size \n",
    "                 mode=\"relative\",             # \"absolute\": SD values. \"relative\": 100*(SD/mu). Default=\"absolute\"\n",
    "                 produce_animation=False,     # Default=False\n",
    "                 animation_name=\"_.mp4\",      # Default=\"./animation.mp4\"\n",
    "                 figsize=(8,8),             # Optional, default=(15,15)\n",
    "                 scalingpower=2,              # Raise SD to this power when using \"absolute\" to scale size. Optional, default=3\n",
    "                 scalingscalar=0.1,           # Multiply SD by this before raising to power. Optional, default=10\n",
    "                 limit=4000,                  # What size to limit the point to (optional, default: 4000). Only for \"relative\" mode             \n",
    "                 legend_num=6,                # How many legend entries to show\n",
    "                 legend_HDI=0.99)             # HDI for setting upper and lower bound of legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae116a85",
   "metadata": {},
   "source": [
    "# Finish up Run in Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ff871",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_mode: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e91d4",
   "metadata": {},
   "source": [
    "# Save Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"../trained_models/for_prediction/MCDropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537bdcc",
   "metadata": {},
   "source": [
    "# Train the Model with More Data Available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b6ac6",
   "metadata": {},
   "source": [
    "## Split the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0658072",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_lims_in2 = [[0., 2.], [0., 2.], [0., 2.], [0., 2.], [0., 2.], [0., 2.]]\n",
    "train_lims_all2 = [[2., 15], [2., 15], [2., 15], [2., 15], [2., 15], [2., 15]]\n",
    "ood_lims_out2 = [[15, 50], [15, 50], [15, 50], [15, 50], [15, 50], [15, 50]]\n",
    "\n",
    "TRAIN2, VAL2, TEST2 = pre.split_by_bounds(df=df_orig,\n",
    "                                 x_cols=x_cols,\n",
    "                                 y_cols=y_cols,\n",
    "                                 train_lims_all=train_lims_all2,\n",
    "                                 ood_lims_in=ood_lims_in2,\n",
    "                                 ood_lims_out=ood_lims_out2,\n",
    "                                 data_mean=data_mean, \n",
    "                                 data_std=data_std, \n",
    "                                 PCA_components=components,\n",
    "                                 val_split=0.1,\n",
    "                                 verbose=True,\n",
    "                                 plots=True,\n",
    "                                 figsize=(10,10))\n",
    "                                      #save_image=False, savename=\"../reports/final_report_images/splitting_data/manual_bounds.eps\", saveformat=\"eps\")\n",
    "\n",
    "x_train2, y_train2, train_data2, train_indices2 = TRAIN2\n",
    "x_val2, y_val2, val_data2, val_indices2 = VAL2\n",
    "x_test2, y_test2, test_data2, test_indices2 = TEST2\n",
    "\n",
    "y_train_unnorm = pre.unnormalise(y_train2,data_mean[y_cols].values,data_std[y_cols].values)\n",
    "y_val_unnorm = pre.unnormalise(y_val2,data_mean[y_cols].values,data_std[y_cols].values)\n",
    "y_test_unnorm = pre.unnormalise(y_test2,data_mean[y_cols].values,data_std[y_cols].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a848df4",
   "metadata": {},
   "source": [
    "## Load the Previous Model Progress and Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568739e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.net.load_state_dict(checkpoint['model'])\n",
    "model.optimiser.load_state_dict(checkpoint['optimiser'])\n",
    "\n",
    "start_epoch2 = checkpoint['epoch']\n",
    "epochs2 = 2000 \n",
    "batch_size2 = len(train_data2)\n",
    "\n",
    "\n",
    "model.wandb_mode = False  # We've already done the training and finished up wandb, so disable this\n",
    "\n",
    "net, train_loss, val_loss = model.retrain_model(train_data2, batch_size2, epochs2, save_checkpoint=True, checkpoint_path=checkpoint_path)\n",
    "\n",
    "print(f\"Ran epoch {start_epoch2} to {start_epoch2 + epochs2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train2, means_train2, stds_train2, y_train_np2 = model.run_sampling(x_train2, y_train2)\n",
    "samples_test2, means_test2, stds_test2, y_test_np2 = model.run_sampling(x_test2, y_test2)             \n",
    "samples_val2, means_val2, stds_val2, y_val_np2 = model.run_sampling(x_val2, y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a53b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train2 = post.count_wrong_preds(samples_train2, y_train_np2, 1, \"SD\")\n",
    "err_test2 = post.count_wrong_preds(samples_test2, y_test_np2, 1, \"SD\")\n",
    "err_val2 = post.count_wrong_preds(samples_val2, y_val_np2, 1, \"SD\")\n",
    "\n",
    "print(f\"Train error:\\t{np.sum(err_train2)}\\tPREVIOUS:\\t{np.sum(err_train)}\")\n",
    "print(f\"Test error:\\t{np.sum(err_test2)}\\tPREVIOUS:\\t{np.sum(err_test)}\")\n",
    "print(f\"Val error:\\t{np.sum(err_val2)}\\tPREVIOUS:\\t{np.sum(err_val)}\")\n",
    "print(\"----------------------\")\n",
    "print(f\"Total error:\\t{np.sum(err_train2)+np.sum(err_test2)+np.sum(err_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00135665",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncert_plot = post.result_plots(all_samples = [samples_val2, samples_train2, samples_test2],\n",
    "             labels = [\"Validation (2)\", \"Train (2)\", \"Test (OoD) (2)\"],\n",
    "             output_labels = [\"freq1\", \"freq2\", \"freq3\"],\n",
    "             all_true = [y_val_np2, y_train_np2, y_test_np2],\n",
    "             output_num=0,\n",
    "             #true_inds = [val_indices, train_indices, test_indices],\n",
    "             interval = 1,\n",
    "             method=\"SD\",\n",
    "             component_name = parameters[\"component\"],\n",
    "             sort=True,\n",
    "             sortby=\"0\",\n",
    "             bar_method=\"bars\",\n",
    "             title=\"1: No true inds, sort=False. Multiple outputs\",\n",
    "             ylabel=\"First Frequency (Hz)\",\n",
    "             figsize=(20,20),\n",
    "             ylim=(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e047e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
